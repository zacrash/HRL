import random

from keras.models import Sequential
from keras.layers import Dense, Activation, Flatten, Convolution2D, MaxPooling2D, LSTM
from keras.optimizers import Adam
from collections import deque
import keras.backend as K
import numpy as np

from algorithms import DQN


class HRL(DQN):
    def __init__(self, state_size, action_size, max_tau=10):
        self.max_tau = max_tau
        DQN.__init__(self, state_size=state_size, action_size=action_size)

    def replay(self, batch_size):
        batch_size = min(self.batch_size, len(self.memory))
        minibatch = random.sample(self.memory, batch_size)
        minibatch = self.relabel(minibatch)
        for i,step in enumerate(minibatch):
            state = step['state']
            next_state = step['nextState']
            goal = step['goal']
            actions = self.model.predict(state)[0]
            y[i] = -1*H(next_state,goal) * (tau[i]==0) \
                   + np.argmax(actions) * (tau[i] != 0)
        


""" Functions:
remember(state,action,reward,nextState,done)
act(state)
replay(batch_size)
get_model()
"""
